# ğŸ—£ï¸ TranscriÃ§Ã£o de ReuniÃµes com SeparaÃ§Ã£o de Participantes (Whisper + Pyannote)

Este script Python realiza a transcriÃ§Ã£o de Ã¡udios ou vÃ­deos com identificaÃ§Ã£o de diferentes participantes (diarizaÃ§Ã£o de locutor), utilizando:

- ğŸ¤ [Whisper](https://github.com/openai/whisper) da OpenAI para transcriÃ§Ã£o automÃ¡tica
- ğŸ§  [pyannote-audio](https://github.com/pyannote/pyannote-audio) para separaÃ§Ã£o de falas por participante
- ğŸ§ Suporte para entrada em `.mp3`, `.wav`, `.mp4`, `.webm` (qualquer formato que o FFmpeg aceite)

---

Boa parde do cÃ³digo foi tirada desse tutorial do canal 1littlecoder:
[link do video](https://www.youtube.com/watch?v=MVW746z8y_I)

## ğŸš€ Funcionalidades

- Converte vÃ­deos ou Ã¡udios para `.wav` automaticamente usando **FFmpeg**
- Transcreve o conteÃºdo do Ã¡udio para texto com **Whisper**
- Separa as falas por participantes usando **clustering de embeddings de voz**
- Gera um arquivo `transcricao_formatada.txt` com o conteÃºdo dividido por `SPEAKER 1`, `SPEAKER 2`, etc.

---

## ğŸ“¦ Requisitos

- Python 3.8+
- GPU com CUDA (recomendado)
- FFmpeg instalado no sistema [Linux](https://www.geeksforgeeks.org/how-to-install-ffmpeg-in-linux/) [Windows](https://www.geeksforgeeks.org/how-to-install-ffmpeg-on-windows/)
- DependÃªncias Python:

```bash
pip install -r requirements.txt
```

ğŸ› ï¸ Como usar
1. Configure o script

No final do arquivo .py, altere estas variÃ¡veis conforme seu caso:

audio_path = 'akita.webm'  # Caminho para o arquivo de Ã¡udio ou vÃ­deo
speakers = 2               # NÃºmero estimado de participantes
model = 'turbo'             # Modelo Whisper: tiny, base, small, medium, large, turbo *escolha conforme a sua GPU

O modelo turbo uso algo entre 6 a 7Gb de VRAM


1. Execute o script
```
python main.py
```

ğŸ§¾ Exemplo de saÃ­da (transcricao_formatada.txt)

```
SPEAKER 1 0:00:03
Bom dia a todos, vamos iniciar a reuniÃ£o.

SPEAKER 2 0:00:10
Tudo certo, podemos seguir com a pauta de hoje.
```

â“ DÃºvidas comuns
ğŸ“Œ Por que o arquivo .wav fica tÃ£o grande?

O .wav Ã© um formato sem compressÃ£o, usado para garantir mÃ¡xima fidelidade de Ã¡udio para os modelos. Ele ocupa mais espaÃ§o, mas Ã© necessÃ¡rio para a precisÃ£o.

ğŸ“Œ O script melhora a qualidade ao converter de .mp3 para .wav?

NÃ£o. A conversÃ£o nÃ£o recupera qualidade perdida, apenas transforma o Ã¡udio para um formato compatÃ­vel com os modelos.

ğŸ“ Melhorias futuras (sugestÃµes) 
GeraÃ§Ã£o de ata automatizada com GPT (via http) ou implementaÃ§Ã£o usando llama

Interface web com upload de arquivos


# ğŸ—£ï¸ Meeting Transcription with Speaker Separation (Whisper + Pyannote)

This Python script performs audio or video transcription with speaker identification (speaker diarization), using:

- ğŸ¤ [Whisper](https://github.com/openai/whisper) by OpenAI for automatic transcription
- ğŸ§  [pyannote-audio](https://github.com/pyannote/pyannote-audio) for separating speech by speaker
- ğŸ§ Supports input formats like `.mp3`, `.wav`, `.mp4`, `.webm` (any format supported by FFmpeg)

---

A good part of the code was adapted from this tutorial by 1littlecoder:  
[Watch the video](https://www.youtube.com/watch?v=MVW746z8y_I)

---

## ğŸš€ Features

- Automatically converts video or audio to `.wav` using **FFmpeg**
- Transcribes audio content to text using **Whisper**
- Separates speech by speaker using **voice embedding clustering**
- Generates a `transcricao_formatada.txt` file with content divided by `SPEAKER 1`, `SPEAKER 2`, etc.

---

## ğŸ“¦ Requirements

- Python 3.8+
- CUDA-enabled GPU (recommended)
- FFmpeg installed on your system ([Linux](https://www.geeksforgeeks.org/how-to-install-ffmpeg-in-linux/) | [Windows](https://www.geeksforgeeks.org/how-to-install-ffmpeg-on-windows/))
- Python dependencies:

```bash
pip install -r requirements.txt
```

ğŸ› ï¸ How to Use
1. Configure the script

At the end of the .py file, adjust these variables according to your setup:

```
audio_path = 'akita.webm'  # Path to your audio or video file
speakers = 2               # Estimated number of participants
model = 'turbo'            # Whisper model: tiny, base, small, medium, large, turbo (choose based on your GPU)
```

ğŸ’¡ The turbo model uses around 6 to 7GB of VRAM

2. Run the script
```
python main.py
```

ğŸ§¾ Example Output (transcricao_formatada.txt)

```
SPEAKER 1 0:00:03
Good morning everyone, let's start the meeting.

SPEAKER 2 0:00:10
All set, we can proceed with today's agenda.
```
â“ Frequently Asked Questions
ğŸ“Œ Why is the .wav file so large?

The .wav format is uncompressed, used to ensure maximum audio fidelity for the models. It takes up more space but is required for accurate processing.

ğŸ“Œ Does the script improve audio quality when converting from .mp3 to .wav?

No. Conversion does not recover lost quality â€” it only transforms the file into a model-compatible format.

ğŸ“ Future Improvements (Ideas)
Automated meeting summary (minutes) generation using GPT (via HTTP) or local model integration like LLaMA

Web interface for file upload